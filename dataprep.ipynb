{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Modified Reference Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>LVEF</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>Cause of death</th>\n",
       "      <th>Exit of the study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>35.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>35.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>39.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>38.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>34.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient_ID  LVEF  Creatinine  Cause of death  Exit of the study\n",
       "0      P0001  35.0       106.0               0                0.0\n",
       "1      P0002  35.0       121.0               0                0.0\n",
       "2      P0003  39.0        87.0               0                0.0\n",
       "3      P0004  38.0        77.0               0                0.0\n",
       "4      P0005  34.0        88.0               0                0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "refTable = pd.read_csv('resources/subject-info.csv')\n",
    "refTable = refTable[['Patient ID', 'LVEF (%)', 'Creatinine (?mol/L)', 'Cause of death', 'Exit of the study']]\n",
    "refTable.rename(columns={'Patient ID': 'Patient_ID', \"LVEF (%)\": \"LVEF\", \"Creatinine (?mol/L)\": \"Creatinine\"}, inplace = True)\n",
    "\n",
    "refTable['LVEF'] = refTable['LVEF'].astype(float)\n",
    "refTable['Creatinine'] = refTable['Creatinine'].astype(float)\n",
    "\n",
    "# Fill NaN values in 'Exit_of_the_study' with 0 (avoiding chained assignment)\n",
    "refTable['Exit of the study'] = refTable['Exit of the study'].fillna(0)\n",
    "\n",
    "# Now, you can filter based on 'Exit_of_the_study' being 0\n",
    "refTable = refTable[refTable['Exit of the study'] == 0]\n",
    "\n",
    "# Exclude patients where 'Cause_of_Death' is not equal to 0\n",
    "refTable = refTable[refTable['Cause of death'] == 0]\n",
    "\n",
    "refTable = refTable.dropna()\n",
    "\n",
    "refTable.index = range(0, len(refTable), 1)\n",
    "refTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "refTable.to_csv('resources/reference-table.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Retrieval and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal processing (the whole dataset is downloaded to local storage with wget) [Employed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wfdb\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_dir):\n",
    "    \"\"\"Sets up a logger that writes to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"{timestamp}.log\"\n",
    "    log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(log_filepath)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for ECG Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(r_signal, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    f_signal = filtfilt(b, a, r_signal)\n",
    "    return f_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_batch(signals, patient_ids, output_dir, filename, sampling_rate, plots_per_image=9):\n",
    "    \"\"\"Plots a batch of ECG signals with grid and patient IDs, and saves them as a single image with a fixed number of plots.\"\"\"\n",
    "    num_signals = len(signals)\n",
    "    if num_signals == 0:\n",
    "        return\n",
    "\n",
    "    rows = 3\n",
    "    cols = 3\n",
    "    time = np.arange(signals[0].shape[0]) / sampling_rate if signals else np.array([])\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(plots_per_image):\n",
    "        if i < num_signals:\n",
    "            axes[i].plot(time, signals[i])\n",
    "            axes[i].grid(True)\n",
    "            axes[i].set_title(f\"Patient ID: {patient_ids[i]}\", fontsize=8)\n",
    "            axes[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "        else:\n",
    "            fig.delaxes(axes[i])  # Remove empty subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing into TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(patient_ids, record_dir, refTable, output_dir, inspection_dir, sampling_rate, window_seconds=5, start_offset_seconds=3600, lead_index=0, inspection_frequency=10, plots_per_image=9, logger=None):\n",
    "    \"\"\"\n",
    "    Writes TFRecords and periodically saves batches of ECG windows for inspection with grid and patient IDs, with a fixed number of plots per image.\n",
    "\n",
    "    Args:\n",
    "        patient_ids (list): List of patient IDs to process.\n",
    "        record_dir (str): Directory containing the WFDB record files.\n",
    "        refTable (pd.DataFrame): DataFrame containing patient information.\n",
    "        output_dir (str): Directory to save the TFRecord files.\n",
    "        inspection_dir (str): Directory to save inspection plot images.\n",
    "        sampling_rate (int): Expected sampling rate of the ECG signals.\n",
    "        window_seconds (int): The size of the window in seconds to extract.\n",
    "        start_offset_seconds (int): Starting offset in seconds.\n",
    "        lead_index (int): The index of the lead to process.\n",
    "        inspection_frequency (int): Save an inspection plot every N processed patients.\n",
    "        plots_per_image (int): The fixed number of plots to include in each inspection image.\n",
    "        logger (logging.Logger, optional): Logger object.\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        def log_info(message):\n",
    "            print(message)\n",
    "        def log_error(message):\n",
    "            print(f\"Error: {message}\")\n",
    "    else:\n",
    "        log_info = logger.info\n",
    "        log_error = logger.error\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(inspection_dir, exist_ok=True)\n",
    "    all_windows_for_inspection = []\n",
    "    inspection_patient_ids = []\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    target_window_size = int(window_seconds * sampling_rate)\n",
    "    start_index = int(start_offset_seconds * sampling_rate)\n",
    "\n",
    "    for i, pid in enumerate(tqdm(patient_ids, desc=f\"Writing TFRecords ({window_seconds} sec, Lead {lead_index})\")):\n",
    "        record_path = f\"{record_dir}/{pid}\"\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_path, channels=[lead_index])\n",
    "            signals = record.p_signal.astype(np.float32)\n",
    "            fields = record.__dict__\n",
    "            fs = fields['fs']\n",
    "            if fs != sampling_rate:\n",
    "                log_info(f\"Sampling rate mismatch for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if signals.shape[1] < 1:\n",
    "                log_info(f\"Less than 1 lead for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if len(signals) < start_index + target_window_size:\n",
    "                log_info(f\"Signal too short for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            window = signals[start_index:start_index + target_window_size, 0]\n",
    "            filtered_signal = butter_bandpass_filter(window, 1, 40, fs, order=5)\n",
    "\n",
    "            patient_row = refTable[refTable['Patient_ID'] == pid]\n",
    "            if patient_row.empty:\n",
    "                log_info(f\"Patient {pid} not in refTable. Skipping.\")\n",
    "                continue\n",
    "            lvef = patient_row['LVEF'].values[0]\n",
    "            creatinine = patient_row['Creatinine'].values[0]\n",
    "\n",
    "            tfrecord_path = os.path.join(output_dir, f\"{pid}_lead_{lead_index}_window_{start_offset_seconds}s_{window_seconds}s.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "                feature = {\n",
    "                    'signal': _float_feature(filtered_signal.astype(np.float32).flatten()),\n",
    "                    'lvef': _float_feature([float(lvef)]),\n",
    "                    'creatinine': _float_feature([float(creatinine)])\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "            log_info(f\"Processed and wrote TFRecord for patient {pid}.\")\n",
    "\n",
    "            all_windows_for_inspection.append(filtered_signal)\n",
    "            inspection_patient_ids.append(pid)\n",
    "\n",
    "            if len(all_windows_for_inspection) >= plots_per_image:\n",
    "                plot_filename = f\"inspection_batch_{i // inspection_frequency + 1}_part_{(len(all_windows_for_inspection) - 1) // plots_per_image + 1}_lead_{lead_index}.png\"\n",
    "                batch_signals = all_windows_for_inspection[:plots_per_image]\n",
    "                batch_pids = inspection_patient_ids[:plots_per_image]\n",
    "                plot_and_save_batch(batch_signals, batch_pids, inspection_dir, plot_filename, sampling_rate, plots_per_image)\n",
    "                all_windows_for_inspection = all_windows_for_inspection[plots_per_image:]\n",
    "                inspection_patient_ids = inspection_patient_ids[plots_per_image:]\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error processing patient {pid}: {e}\")\n",
    "\n",
    "    # Save any remaining windows\n",
    "    if all_windows_for_inspection:\n",
    "        plot_filename = f\"inspection_batch_final_lead_{lead_index}.png\"\n",
    "        plot_and_save_batch(all_windows_for_inspection, inspection_patient_ids, inspection_dir, plot_filename, sampling_rate, plots_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing TFRecords (5 sec, Lead 1): 100%|██████████| 695/695 [12:44<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    data_prep_dir = \"./data_preparation\"\n",
    "    os.makedirs(data_prep_dir, exist_ok=True)\n",
    "    log_directory = os.path.join(data_prep_dir, \"logs\")\n",
    "    inspection_directory = os.path.join(data_prep_dir, \"inspection_plots\")\n",
    "\n",
    "    logger = setup_logger(log_directory)\n",
    "    logger.info(\"Starting TFRecord generation with inspection (9 plots per image).\")\n",
    "\n",
    "    refTable = pd.read_csv('resources/reference-table.csv')\n",
    "    all_patients = refTable['Patient_ID'].tolist()\n",
    "    num_patients = len(all_patients)\n",
    "\n",
    "    record_directory = \"F:\\\\physionet.org\\\\files\\\\music-sudden-cardiac-death\\\\1.0.1\\\\Holter_ECG\"\n",
    "    output_directory = os.path.join(data_prep_dir, \"tfrecords-5seconds-singlelead\")\n",
    "    reference_table = refTable\n",
    "    sampling_rate = 200\n",
    "    train_patient_ids = all_patients[:num_patients]\n",
    "    lead_to_process = 1 #Index of lead\n",
    "    window_size_seconds = 5\n",
    "    inspection_frequency = 5\n",
    "    plots_per_image = 9  # Set the desired number of plots per image\n",
    "\n",
    "    write_tfrecords(train_patient_ids, record_directory, reference_table, output_directory, inspection_directory, sampling_rate, window_size_seconds, lead_index=lead_to_process, inspection_frequency=inspection_frequency, plots_per_image=plots_per_image, logger=logger)\n",
    "\n",
    "    logger.info(\"TFRecord generation with inspection (9 plots per image) completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your file is in a format that wfdb can read (e.g., .dat and .hea from PhysioNet)\n",
    "record = wfdb.rdrecord('your_ecg_file_base_name', channels=[0]) # Read the first channel\n",
    "signal = record.p_signal\n",
    "time = np.arange(signal.shape[0]) / record.fs\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time, signal)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "plt.title('ECG Signal')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(record) # Print record information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In place signal retrieval and processing from Physionet database [Alternative]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
