{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Modified Reference Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>LVEF</th>\n",
       "      <th>Cause of death</th>\n",
       "      <th>Exit of the study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient_ID  LVEF  Cause of death  Exit of the study\n",
       "0      P0001  35.0               0                0.0\n",
       "1      P0002  35.0               0                0.0\n",
       "2      P0003  39.0               0                0.0\n",
       "3      P0004  38.0               0                0.0\n",
       "4      P0005  34.0               0                0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "refTable = pd.read_csv('resources/subject-info.csv')\n",
    "refTable = refTable[['Patient ID', 'LVEF (%)', 'Cause of death', 'Exit of the study']]\n",
    "refTable.rename(columns={'Patient ID': 'Patient_ID', \"LVEF (%)\": \"LVEF\"}, inplace = True)\n",
    "\n",
    "refTable['LVEF'] = refTable['LVEF'].astype(float)\n",
    "\n",
    "# Fill NaN values in 'Exit_of_the_study' with 0 (avoiding chained assignment)\n",
    "refTable['Exit of the study'] = refTable['Exit of the study'].fillna(0)\n",
    "\n",
    "# Now, you can filter based on 'Exit_of_the_study' being 0\n",
    "refTable = refTable[refTable['Exit of the study'] == 0]\n",
    "\n",
    "# Exclude patients where 'Cause_of_Death' is not equal to 0\n",
    "refTable = refTable[refTable['Cause of death'] == 0]\n",
    "\n",
    "refTable = refTable.dropna()\n",
    "\n",
    "refTable.index = range(0, len(refTable), 1)\n",
    "refTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "refTable.to_csv('resources/reference-table.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Retrieval and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal processing (the whole dataset is downloaded to local storage with wget) [Employed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wfdb\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_dir):\n",
    "    \"\"\"Sets up a logger that writes to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"{timestamp}.log\"\n",
    "    log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(log_filepath)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for ECG Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(r_signal, lowcut, highcut, fs, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    f_signal = filtfilt(b, a, r_signal)\n",
    "    return f_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_batch(signals, patient_ids, output_dir, filename, sampling_rate, plots_per_image=9):\n",
    "    \"\"\"Plots a batch of ECG signals with grid and patient IDs, and saves them as a single image with a fixed number of plots.\"\"\"\n",
    "    num_signals = len(signals)\n",
    "    if num_signals == 0:\n",
    "        return\n",
    "\n",
    "    rows = 3\n",
    "    cols = 3\n",
    "    time = np.arange(signals[0].shape[0]) / sampling_rate if signals else np.array([])\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(plots_per_image):\n",
    "        if i < num_signals:\n",
    "            axes[i].plot(time, signals[i])\n",
    "            axes[i].grid(True)\n",
    "            axes[i].set_title(f\"Patient ID: {patient_ids[i]}\", fontsize=8)\n",
    "            axes[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "        else:\n",
    "            fig.delaxes(axes[i])  # Remove empty subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing into TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(patient_ids, record_dir, refTable, output_dir, inspection_dir, sampling_rate, window_seconds=5, start_offset_seconds=5, lead_index=0, inspection_frequency=10, plots_per_image=9, logger=None):\n",
    "    \"\"\"\n",
    "    Writes TFRecords and periodically saves batches of ECG windows for inspection with grid and patient IDs, with a fixed number of plots per image.\n",
    "\n",
    "    Args:\n",
    "        patient_ids (list): List of patient IDs to process.\n",
    "        record_dir (str): Directory containing the WFDB record files.\n",
    "        refTable (pd.DataFrame): DataFrame containing patient information.\n",
    "        output_dir (str): Directory to save the TFRecord files.\n",
    "        inspection_dir (str): Directory to save inspection plot images.\n",
    "        sampling_rate (int): Expected sampling rate of the ECG signals.\n",
    "        window_seconds (int): The size of the window in seconds to extract.\n",
    "        start_offset_seconds (int): Starting offset in seconds.\n",
    "        lead_index (int): The index of the lead to process.\n",
    "        inspection_frequency (int): Save an inspection plot every N processed patients.\n",
    "        plots_per_image (int): The fixed number of plots to include in each inspection image.\n",
    "        logger (logging.Logger, optional): Logger object.\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        def log_info(message):\n",
    "            print(message)\n",
    "        def log_error(message):\n",
    "            print(f\"Error: {message}\")\n",
    "    else:\n",
    "        log_info = logger.info\n",
    "        log_error = logger.error\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(inspection_dir, exist_ok=True)\n",
    "    all_windows_for_inspection = []\n",
    "    inspection_patient_ids = []\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    target_window_size = int(window_seconds * sampling_rate)\n",
    "    start_index = int(start_offset_seconds * sampling_rate)\n",
    "\n",
    "    for i, pid in enumerate(tqdm(patient_ids, desc=f\"Writing TFRecords ({window_seconds} sec, Lead {lead_index})\")):\n",
    "        record_path = f\"{record_dir}/{pid}_H\"\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_path, channels=[lead_index])\n",
    "            signals = record.p_signal.astype(np.float32)\n",
    "            fields = record.__dict__\n",
    "            fs = fields['fs']\n",
    "            if fs != sampling_rate:\n",
    "                log_info(f\"Sampling rate mismatch for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if signals.shape[1] < 1:\n",
    "                log_info(f\"Less than 1 lead for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if len(signals) < start_index + target_window_size:\n",
    "                log_info(f\"Signal too short for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            window = signals[start_index:start_index + target_window_size, 0]\n",
    "            \n",
    "            patient_row = refTable[refTable['Patient_ID'] == pid]\n",
    "            if patient_row.empty:\n",
    "                log_info(f\"Patient {pid} not in refTable. Skipping.\")\n",
    "                continue\n",
    "            lvef = patient_row['LVEF'].values[0]\n",
    "\n",
    "            tfrecord_path = os.path.join(output_dir, f\"{pid}_lead_{lead_index}_window_{start_offset_seconds}s_{window_seconds}s.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "                feature = {\n",
    "                    'signal': _float_feature(window.astype(np.float32).flatten()), # Use 'window' directly\n",
    "                    'lvef': _float_feature([float(lvef)]),\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "            log_info(f\"Processed and wrote TFRecord for patient {pid}.\")\n",
    "\n",
    "            all_windows_for_inspection.append(window) # Append 'window' directly\n",
    "            inspection_patient_ids.append(pid)\n",
    "\n",
    "            if len(all_windows_for_inspection) >= plots_per_image:\n",
    "                plot_filename = f\"inspection_batch_{i // inspection_frequency + 1}_part_{(len(all_windows_for_inspection) - 1) // plots_per_image + 1}_lead_{lead_index}.png\"\n",
    "                batch_signals = all_windows_for_inspection[:plots_per_image]\n",
    "                batch_pids = inspection_patient_ids[:plots_per_image]\n",
    "                plot_and_save_batch(batch_signals, batch_pids, inspection_dir, plot_filename, sampling_rate, plots_per_image)\n",
    "                all_windows_for_inspection = all_windows_for_inspection[plots_per_image:]\n",
    "                inspection_patient_ids = inspection_patient_ids[plots_per_image:]\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error processing patient {pid}: {e}\")\n",
    "\n",
    "    # Save any remaining windows\n",
    "    if all_windows_for_inspection:\n",
    "        plot_filename = f\"inspection_batch_final_lead_{lead_index}.png\"\n",
    "        plot_and_save_batch(all_windows_for_inspection, inspection_patient_ids, inspection_dir, plot_filename, sampling_rate, plots_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "import pywt # For wavelet transform\n",
    "import os # Added for file operations in write_tfrecords\n",
    "import tensorflow as tf # Added for TFRecord operations\n",
    "import wfdb # Added for WFDB record reading\n",
    "from tqdm import tqdm # Added for progress bar\n",
    "\n",
    "sampling_rate = 1000 # Hz\n",
    "duration = 5 # seconds\n",
    "num_samples = sampling_rate * duration\n",
    "time = np.linspace(0, duration, num_samples, endpoint=False)\n",
    "\n",
    "# Simulate a basic ECG-like signal (sum of sinusoids for QRS, P, T waves)\n",
    "ecg_freq = 1.2 # Approximate heart rate in Hz (72 bpm)\n",
    "ecg_signal = 0.6 * np.sin(2 * np.pi * ecg_freq * time) \\\n",
    "             + 0.2 * np.sin(2 * np.pi * 3 * ecg_freq * time) \\\n",
    "             + 0.1 * np.sin(2 * np.pi * 5 * ecg_freq * time)\n",
    "\n",
    "# Add baseline wander (low-frequency noise)\n",
    "baseline_wander_freq = 0.2 # Hz\n",
    "baseline_wander = 0.15 * np.sin(2 * np.pi * baseline_wander_freq * time)\n",
    "\n",
    "# Add powerline interference (e.g., 50 Hz or 60 Hz)\n",
    "powerline_freq = 50 # Hz (adjust to 60 Hz if in regions like North America)\n",
    "powerline_noise = 0.1 * np.sin(2 * np.pi * powerline_freq * time + np.random.rand() * 2 * np.pi)\n",
    "\n",
    "# Add high-frequency random noise (e.g., muscle artifacts, instrumentation noise)\n",
    "random_noise = 0.05 * np.random.randn(num_samples)\n",
    "\n",
    "# Combine to create the noisy ECG signal\n",
    "noisy_ecg = ecg_signal + baseline_wander + powerline_noise + random_noise\n",
    "\n",
    "# --- 2. Denoising Algorithm Steps (Standalone Example) ---\n",
    "# This section remains as a standalone example of the denoising process.\n",
    "\n",
    "# Step 2.1: Baseline Wander Removal (High-Pass Filter)\n",
    "cutoff_highpass = 0.5 # Hz\n",
    "nyquist = 0.5 * sampling_rate\n",
    "normal_cutoff_highpass = cutoff_highpass / nyquist\n",
    "b_hp, a_hp = butter(2, normal_cutoff_highpass, btype='high', analog=False)\n",
    "ecg_no_baseline = filtfilt(b_hp, a_hp, noisy_ecg)\n",
    "\n",
    "# Step 2.2: Powerline Interference Removal (Notch Filter)\n",
    "notch_freq = powerline_freq # Frequency to remove (Hz)\n",
    "quality_factor = 30 # Q-factor, determines the bandwidth of the notch filter\n",
    "b_notch, a_notch = iirnotch(notch_freq, quality_factor, sampling_rate)\n",
    "ecg_no_powerline = filtfilt(b_notch, a_notch, ecg_no_baseline)\n",
    "\n",
    "# Step 2.3: High-Frequency Noise Removal (Wavelet Denoising)\n",
    "wavelet = 'db4'\n",
    "decomposition_level = 4\n",
    "coeffs = pywt.wavedec(ecg_no_powerline, wavelet, level=decomposition_level)\n",
    "threshold = 0.5 * np.std(coeffs[-1]) * np.sqrt(2 * np.log(len(ecg_no_powerline)))\n",
    "denoised_coeffs = [coeffs[0]]\n",
    "for i in range(1, len(coeffs)):\n",
    "    denoised_coeffs.append(pywt.threshold(coeffs[i], threshold, mode='soft'))\n",
    "final_denoised_ecg = pywt.waverec(denoised_coeffs, wavelet)\n",
    "\n",
    "if len(final_denoised_ecg) > len(noisy_ecg):\n",
    "    final_denoised_ecg = final_denoised_ecg[:len(noisy_ecg)]\n",
    "elif len(final_denoised_ecg) < len(noisy_ecg):\n",
    "    padding = np.zeros(len(noisy_ecg) - len(final_denoised_ecg))\n",
    "    final_denoised_ecg = np.concatenate((final_denoised_ecg, padding))\n",
    "\n",
    "\n",
    "# --- Integrated write_tfrecords function with Denoising ---\n",
    "def write_tfrecords(patient_ids, record_dir, refTable, output_dir, inspection_dir, sampling_rate, window_seconds=5, start_offset_seconds=5, lead_index=0, inspection_frequency=10, plots_per_image=9, logger=None,\n",
    "                    denoising_params=None):\n",
    "    \"\"\"\n",
    "    Writes TFRecords and periodically saves batches of ECG windows for inspection with grid and patient IDs,\n",
    "    with a fixed number of plots per image. Integrates ECG denoising.\n",
    "\n",
    "    Args:\n",
    "        patient_ids (list): List of patient IDs to process.\n",
    "        record_dir (str): Directory containing the WFDB record files.\n",
    "        refTable (pd.DataFrame): DataFrame containing patient information.\n",
    "        output_dir (str): Directory to save the TFRecord files.\n",
    "        inspection_dir (str): Directory to save inspection plot images.\n",
    "        sampling_rate (int): Expected sampling rate of the ECG signals.\n",
    "        window_seconds (int): The size of the window in seconds to extract.\n",
    "        start_offset_seconds (int): Starting offset in seconds.\n",
    "        lead_index (int): The index of the lead to process.\n",
    "        inspection_frequency (int): Save an inspection plot every N processed patients.\n",
    "        plots_per_image (int): The fixed number of plots to include in each inspection image.\n",
    "        logger (logging.Logger, optional): Logger object.\n",
    "        denoising_params (dict, optional): Dictionary of denoising parameters.\n",
    "                                          Expected keys: 'powerline_freq', 'cutoff_highpass',\n",
    "                                          'notch_quality_factor', 'wavelet', 'decomposition_level'.\n",
    "                                          If None, default values will be used.\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        def log_info(message):\n",
    "            print(message)\n",
    "        def log_error(message):\n",
    "            print(f\"Error: {message}\")\n",
    "    else:\n",
    "        log_info = logger.info\n",
    "        log_error = logger.error\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(inspection_dir, exist_ok=True)\n",
    "    all_windows_for_inspection = []\n",
    "    inspection_patient_ids = []\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    target_window_size = int(window_seconds * sampling_rate)\n",
    "    start_index = int(start_offset_seconds * sampling_rate)\n",
    "\n",
    "    # Denoising parameters (default values if not provided)\n",
    "    default_denoising_params = {\n",
    "        'powerline_freq': 50,\n",
    "        'cutoff_highpass': 0.5,\n",
    "        'notch_quality_factor': 30,\n",
    "        'wavelet': 'db4',\n",
    "        'decomposition_level': 4\n",
    "    }\n",
    "    params = denoising_params if denoising_params is not None else default_denoising_params\n",
    "\n",
    "    for i, pid in enumerate(tqdm(patient_ids, desc=f\"Writing TFRecords ({window_seconds} sec, Lead {lead_index})\")):\n",
    "        record_path = f\"{record_dir}/{pid}_H\"\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_path, channels=[lead_index])\n",
    "            signals = record.p_signal.astype(np.float32)\n",
    "            fields = record.__dict__\n",
    "            fs = fields['fs']\n",
    "            if fs != sampling_rate:\n",
    "                log_info(f\"Sampling rate mismatch for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if signals.shape[1] < 1:\n",
    "                log_info(f\"Less than 1 lead for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "            if len(signals) < start_index + target_window_size:\n",
    "                log_info(f\"Signal too short for patient {pid}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            window = signals[start_index:start_index + target_window_size, 0]\n",
    "            \n",
    "            # --- Apply Denoising to the extracted window ---\n",
    "            \n",
    "            # Step 1: Baseline Wander Removal (High-Pass Filter)\n",
    "            nyquist = 0.5 * sampling_rate\n",
    "            normal_cutoff_highpass = params['cutoff_highpass'] / nyquist\n",
    "            b_hp, a_hp = butter(2, normal_cutoff_highpass, btype='high', analog=False)\n",
    "            window_denoised_hp = filtfilt(b_hp, a_hp, window)\n",
    "\n",
    "            # Step 2: Powerline Interference Removal (Notch Filter)\n",
    "            b_notch, a_notch = iirnotch(params['powerline_freq'], params['notch_quality_factor'], sampling_rate)\n",
    "            window_denoised_pl = filtfilt(b_notch, a_notch, window_denoised_hp)\n",
    "\n",
    "            # Step 3: High-Frequency Noise Removal (Wavelet Denoising)\n",
    "            coeffs = pywt.wavedec(window_denoised_pl, params['wavelet'], level=params['decomposition_level'])\n",
    "            # Universal threshold calculation\n",
    "            threshold = 0.5 * np.std(coeffs[-1]) * np.sqrt(2 * np.log(len(window_denoised_pl)))\n",
    "            \n",
    "            denoised_coeffs = [coeffs[0]] # Keep approximation coefficients\n",
    "            for k in range(1, len(coeffs)):\n",
    "                denoised_coeffs.append(pywt.threshold(coeffs[k], threshold, mode='soft'))\n",
    "            \n",
    "            cleaned_window = pywt.waverec(denoised_coeffs, params['wavelet'])\n",
    "\n",
    "            # Ensure the length matches the original window length after reconstruction\n",
    "            if len(cleaned_window) > len(window):\n",
    "                cleaned_window = cleaned_window[:len(window)]\n",
    "            elif len(cleaned_window) < len(window):\n",
    "                padding = np.zeros(len(window) - len(cleaned_window))\n",
    "                cleaned_window = np.concatenate((cleaned_window, padding))\n",
    "\n",
    "            # --- End Denoising Application ---\n",
    "\n",
    "            patient_row = refTable[refTable['Patient_ID'] == pid]\n",
    "            if patient_row.empty:\n",
    "                log_info(f\"Patient {pid} not in refTable. Skipping.\")\n",
    "                continue\n",
    "            lvef = patient_row['LVEF'].values[0]\n",
    "\n",
    "            tfrecord_path = os.path.join(output_dir, f\"{pid}_lead_{lead_index}_window_{start_offset_seconds}s_{window_seconds}s.tfrecord\")\n",
    "            with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "                feature = {\n",
    "                    'signal': _float_feature(cleaned_window.astype(np.float32).flatten()), # Use cleaned_window\n",
    "                    'lvef': _float_feature([float(lvef)]),\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "            log_info(f\"Processed and wrote TFRecord for patient {pid}.\")\n",
    "\n",
    "            all_windows_for_inspection.append(cleaned_window) # Append cleaned_window\n",
    "            inspection_patient_ids.append(pid)\n",
    "\n",
    "            if len(all_windows_for_inspection) >= plots_per_image:\n",
    "                plot_filename = f\"inspection_batch_{i // inspection_frequency + 1}_part_{(len(all_windows_for_inspection) - 1) // plots_per_image + 1}_lead_{lead_index}.png\"\n",
    "                batch_signals = all_windows_for_inspection[:plots_per_image]\n",
    "                batch_pids = inspection_patient_ids[:plots_per_image]\n",
    "                plot_and_save_batch(batch_signals, batch_pids, inspection_dir, plot_filename, sampling_rate, plots_per_image)\n",
    "                all_windows_for_inspection = all_windows_for_inspection[plots_per_image:]\n",
    "                inspection_patient_ids = inspection_patient_ids[plots_per_image:]\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error processing patient {pid}: {e}\")\n",
    "\n",
    "    # Save any remaining windows\n",
    "    if all_windows_for_inspection:\n",
    "        plot_filename = f\"inspection_batch_final_lead_{lead_index}.png\"\n",
    "        plot_and_save_batch(all_windows_for_inspection, inspection_patient_ids, inspection_dir, plot_filename, sampling_rate, plots_per_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing TFRecords (5 sec, Lead 0): 100%|██████████| 695/695 [01:14<00:00,  9.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    data_prep_dir = \"./data_preparation\"\n",
    "    os.makedirs(data_prep_dir, exist_ok=True)\n",
    "    log_directory = os.path.join(data_prep_dir, \"logs_highres\")\n",
    "    inspection_directory = os.path.join(data_prep_dir, \"inspection_plots_highres\")\n",
    "\n",
    "    logger = setup_logger(log_directory)\n",
    "    logger.info(\"Starting TFRecord generation with inspection (9 plots per image).\")\n",
    "\n",
    "    refTable = pd.read_csv('resources/reference-table.csv')\n",
    "    all_patients = refTable['Patient_ID'].tolist()\n",
    "    num_patients = len(all_patients)\n",
    "\n",
    "    record_directory = \"F:\\\\physionet.org\\\\files\\\\music-sudden-cardiac-death\\\\1.0.1\\\\High-resolution_ECG\"\n",
    "    output_directory = os.path.join(data_prep_dir, \"tfrecords-5seconds-singlelead-highres\")\n",
    "    reference_table = refTable\n",
    "    sampling_rate = 1000\n",
    "    train_patient_ids = all_patients[:num_patients]\n",
    "    lead_to_process = 0 #Index of lead\n",
    "    window_size_seconds = 5\n",
    "    inspection_frequency = 5\n",
    "    plots_per_image = 9  # Set the desired number of plots per image\n",
    "\n",
    "    write_tfrecords(train_patient_ids, record_directory, reference_table, output_directory, inspection_directory, sampling_rate, window_size_seconds, lead_index=lead_to_process, inspection_frequency=inspection_frequency, plots_per_image=plots_per_image, logger=logger)\n",
    "\n",
    "    logger.info(\"TFRecord generation with inspection (9 plots per image) completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
